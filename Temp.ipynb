{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.preprocessing as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import AugementedConvLSTM\n",
    "import configparser\n",
    "import argparse\n",
    "import h5py\n",
    "import glob\n",
    "import random\n",
    "projection_dimensions = [50,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data = data - data.mean()\n",
    "    data = data / data.std()\n",
    "    return data\n",
    "\n",
    "def set_data(X, Y,):\n",
    "    channel=7\n",
    "    X_normalized = np.zeros((int(channel), np.max(X.shape), int(projection_dimensions[0]), int(projection_dimensions[1])))\n",
    "    for i in range(7):\n",
    "        X_normalized[i,] = normalize(X[i,])\n",
    "    Y_normalized = normalize(Y)\n",
    "    std_observed = Y.std()  \n",
    "    X = X_normalized.transpose(1,2,3,0)\n",
    "    Y = Y_normalized.reshape(-1,projection_dimensions[0], projection_dimensions[1], 1)\n",
    "    return X, Y, std_observed\n",
    "\n",
    "def data_generator(X,Y):\n",
    "    min_train_year = 1948\n",
    "    max_train_year = 1999\n",
    "    min_test_year = 2000\n",
    "    max_test_year = 2005\n",
    "    total_years = max_test_year - min_train_year + 1\n",
    "    train_years = max_train_year - min_train_year + 1\n",
    "    n_days = np.max(X.shape)\n",
    "    train_days = int((n_days/total_years)*train_years)\n",
    "    train_x, train_y = X[:train_days], Y[:train_days]\n",
    "    test_x, test_y = X[train_days:], Y[train_days:]\n",
    "    time_steps = 4\n",
    "    batch_size1 = 15\n",
    "    train_generator = prep.sequence.TimeseriesGenerator(\n",
    "        train_x, \n",
    "        train_y.reshape(-1, projection_dimensions[0], projection_dimensions[1], 1),\n",
    "        length=time_steps, \n",
    "        batch_size=batch_size1\n",
    "        )\n",
    "    test_generator = prep.sequence.TimeseriesGenerator(\n",
    "        test_x, \n",
    "        test_y.reshape(-1, projection_dimensions[0], projection_dimensions[1], 1),\n",
    "        length=time_steps, \n",
    "        batch_size=batch_size1\n",
    "        )\n",
    "    return train_generator, test_generator\n",
    "\n",
    "def train(clstm_model, train_generator, test_generator, load_weights = False, std_observed = 1.0):\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    def actual_rmse_loss(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square((y_pred - y_true)*std_observed)))\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    clstm_model.compile(optimizer=adam, loss=root_mean_squared_error, metrics=[root_mean_squared_error, actual_rmse_loss])\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"convlstm_weights_pr.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=f\"./Graphs/norm_csltm_pre_Graph\", histogram_freq=0, write_graph=True, write_images=False)\n",
    "    termnan = tf.keras.callbacks.TerminateOnNaN()\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_delta=0.005, min_lr=0.000004, verbose=1)    \n",
    "    callbacks_list = [checkpoint,tensorboard, reduce_lr, termnan]\n",
    "    history = clstm_model.fit(\n",
    "        train_generator, \n",
    "        callbacks=callbacks_list, \n",
    "        epochs=32, \n",
    "        validation_data=test_generator,\n",
    "        verbose=1\n",
    "        )\n",
    "    return history\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "    \n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 10:10:48.450765: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-06 10:10:51.391606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 8674 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:18:00.0, compute capability: 7.5\n",
      "2022-07-06 10:10:51.393529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:1 with 9631 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2022-07-06 10:10:51.394790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:2 with 9631 MB memory:  -> device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print([x.name for x in local_device_protos if x.device_type == 'GPU'])\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 10:10:55.214965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8674 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:18:00.0, compute capability: 7.5\n",
      "2022-07-06 10:10:55.216746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9631 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2022-07-06 10:10:55.218381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9631 MB memory:  -> device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:86:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "Aug_ConvLSTM_model = AugementedConvLSTM(\n",
    "    projection_height = projection_dimensions[0], \n",
    "    projection_width = projection_dimensions[1],\n",
    "    timesteps=4\n",
    "    )\n",
    "model = Aug_ConvLSTM_model.model(\n",
    "    [32, 16, 16], \n",
    "    [9,5,3], \n",
    "    [64,32,1], \n",
    "    [9,3,5], \n",
    "    2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  def __init__(self, list_examples, batch_size=15, dim=(50, 50), shuffle=True):\n",
    "    # Constructor of the data generator.\n",
    "    self.dim = dim\n",
    "    self.batch_size = batch_size\n",
    "    self.list_examples = list_examples\n",
    "    self.shuffle = shuffle\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __len__(self):\n",
    "    # Denotes the number of batches per epoch\n",
    "    return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Generate one batch of data\n",
    "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    # Find list of IDs\n",
    "    list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "    # Generate data\n",
    "    X, y = self.__data_generation(list_IDs_temp)\n",
    "    return X, y\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    # This function is called at the end of each epoch.\n",
    "    self.indexes = np.arange(len(self.list_examples))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "\n",
    "  def __data_generation(self, list_IDs_temp):\n",
    "    # Load individual numpy arrays and aggregate them to a batch.\n",
    "    X = np.empty([7, self.batch_size, self.dim[0], self.dim[1]], dtype=np.float32)\n",
    "    y = np.empty([1, self.batch_size, self.dim[0], self.dim[1]], dtype=np.float32)\n",
    "    # # Generate data.\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        # Load sample\n",
    "        X[:,i,:, :] = np.load(ID[0])\n",
    "        # Load labels       \n",
    "        y[:,i,:,:] = np.load(ID[1])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob(os.path.join('Data', 'MIROC-ESM', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(data)\n",
    "\n",
    "labels = glob.glob(os.path.join('Data', 'IMD', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(labels)\n",
    "\n",
    "train_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(train_examples)\n",
    "\n",
    "partition = {}\n",
    "partition['train'] = train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob(os.path.join('Data', 'MIROC-ESM', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(data)\n",
    "\n",
    "labels = glob.glob(os.path.join('Data', 'IMD', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(labels)\n",
    "\n",
    "validation_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(validation_examples)\n",
    "\n",
    "partition['validation'] = validation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (50, 50),\n",
    "          'batch_size': 15,\n",
    "          'timesteps' : 4,\n",
    "          'shuffle': True}\n",
    "# Define the generators\n",
    "training_generator = DataGenerator(partition['train'], **params)\n",
    "validation_generator = DataGenerator(partition['validation'], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35030/4021649041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m '''\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Sarth-tf/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Sarth-tf/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35030/4248850718.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs_temp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35030/4248850718.py\u001b[0m in \u001b[0;36m__data_generation\u001b[0;34m(self, list_IDs_temp)\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# X[i, j, :, :, :] = np.load(self.list_examples[ID_idx[0]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Sarth-tf/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(15, 4, 129, 135, 7) (15, 129, 135, 1)\n",
    "\n",
    "(7, 15, 50, 50) => (1, 7, 15, 50, 50) => ()\n",
    "(1, 15, 50, 50) => (15, 50, 50, 1)\n",
    "\n",
    "(7, 15, 50, 50) (1, 15, 50, 50)\n",
    "'''\n",
    "import itertools\n",
    "for xx, yy in itertools.islice(training_generator, 0, 1, 1):\n",
    "    print(xx.shape, yy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mean of GCM Data:  4.240703115213778e-05\n",
    "Variance of GCM Data:  7.390980757496945e-05\n",
    "Mean of Obseved Data:  0.8937610799332989\n",
    "Variance of Obseved Data:  5.908736811650466\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(model = None):\n",
    "    X, Y = None, None\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4, 50, 50, 7) (15, 50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  def __init__(self, list_examples, batch_size=15, timesteps = 4, n_channels = 7, dim=(50, 50), shuffle=True):\n",
    "    # Constructor of the data generator.\n",
    "    self.dim = dim\n",
    "    self.batch_size = batch_size\n",
    "    self.timesteps = timesteps\n",
    "    self.n_channels = n_channels\n",
    "    self.list_examples = list_examples\n",
    "    self.shuffle = shuffle\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __len__(self):\n",
    "    # Denotes the number of batches per epoch\n",
    "    return int(np.floor(len(self.list_examples) / (self.timesteps*self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Generate one batch of data\n",
    "    indexes = self.indexes[index*self.batch_size*self.timesteps:(index+1)*self.batch_size*self.timesteps]\n",
    "    # Find list of IDs\n",
    "    list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "    # Generate data\n",
    "    assert len(list_IDs_temp) == self.timesteps*self.batch_size\n",
    "    X, y = self.__data_generation(list_IDs_temp)\n",
    "    return X, y\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    # This function is called at the end of each epoch.\n",
    "    self.indexes = np.arange(len(self.list_examples))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "\n",
    "  def __data_generation(self, list_IDs_temp):\n",
    "    # Load individual numpy arrays and aggregate them to a batch.\n",
    "    X = np.empty([self.batch_size, self.timesteps, self.n_channels, self.dim[0], self.dim[1]], dtype=np.float32)\n",
    "    y = np.empty([self.batch_size, 1, self.dim[0], self.dim[1]], dtype=np.float32)\n",
    "    # # Generate data.\n",
    "    # for i, ID in enumerate(np.array(list_IDs_temp)):\n",
    "    #     X[i, :, :, :] = np.load([ID[0]])\n",
    "    #     if (i+1)%4 == 0:\n",
    "    #       y[i, :, :, :, :] = np.load([ID[1]])\n",
    "\n",
    "    # for i, ID in enumerate(np.array(list_IDs_temp).reshape(self.batch_size,self.timesteps,2)):\n",
    "    #   for j, ID_idx in enumerate(ID):\n",
    "    #     # X[i, j, :, :, :] = np.load(self.list_examples[ID_idx[0]])\n",
    "    #     X[i, j, :, :, :] = np.load([ID_idx[0]])\n",
    "    #   y[i, :, :, :, :] = np.load([ID_idx[1]])\n",
    "    j = 0\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        # Load sample\n",
    "        X[j,(i)%self.timesteps,:,:,:] = np.load(ID[0])\n",
    "        # Load labels  \n",
    "        if (i+1)%self.timesteps == 0 and i != 0:\n",
    "          y[j,:,:,:] = np.load(ID[1])\n",
    "          j += 1\n",
    "\n",
    "    return X.transpose(0,1,3,4,2), y.transpose(0,2,3,1)\n",
    "\n",
    "params = {'dim': (50, 50),\n",
    "          'batch_size': 15,\n",
    "          'timesteps' : 4,\n",
    "          'shuffle': True}\n",
    "# Define the generators\n",
    "training_generator = DataGenerator(partition['train'], **params)\n",
    "validation_generator = DataGenerator(partition['validation'], **params)\n",
    "\n",
    "import itertools\n",
    "for xx, yy in itertools.islice(training_generator, 0, 1, 1):\n",
    "    print(xx.shape, yy.shape)\n",
    "# (15, 4, 129, 135, 7) (15, 129, 135, 1)\n",
    "# (15, 4, 50, 50, 7) (15, 50, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(\n",
    "    model,\n",
    "    training_generator, \n",
    "    validation_generator,\n",
    "    load_weights = True,\n",
    "    std_observed = 1.0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('Sarth-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7045c220f384a918686f71b3c00acdc45661fbf73a30bf379cbbfb71cfa47811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
