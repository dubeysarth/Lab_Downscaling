{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.preprocessing as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import AugementedConvLSTM\n",
    "import configparser\n",
    "import argparse\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(model = None):\n",
    "    X, Y = None, None\n",
    "    return X,Y\n",
    "\n",
    "projection_dimensions = [50,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data = data - data.mean()\n",
    "    data = data / data.std()\n",
    "    return data\n",
    "\n",
    "def set_data(X, Y,):\n",
    "    channel=7\n",
    "    X_normalized = np.zeros((int(channel), np.max(X.shape), int(projection_dimensions[0]), int(projection_dimensions[1])))\n",
    "\n",
    "    for i in range(7):\n",
    "        X_normalized[i,] = normalize(X[i,])\n",
    "\n",
    "    Y_normalized = normalize(Y)\n",
    "\n",
    "    print(\"Mean of GCM Data: \",X[0,].mean())\n",
    "    print(\"Variance of GCM Data: \",X[0,].std(),end=\"\\n\")\n",
    "\n",
    "    print(\"Mean of Obseved Data: \",Y.mean())\n",
    "    print(\"Variance of Obseved Data: \",Y.std(),end=\"\\n\")\n",
    "\n",
    "    std_observed = Y.std()  \n",
    "\n",
    "    X = X_normalized.transpose(1,2,3,0)\n",
    "    Y = Y_normalized.reshape(-1,projection_dimensions[0], projection_dimensions[1], 1)\n",
    "    return X, Y, std_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X,Y):\n",
    "    min_train_year = 1948\n",
    "    max_train_year = 1999\n",
    "    min_test_year = 2000\n",
    "    max_test_year = 2005\n",
    "    total_years = max_test_year - min_train_year + 1\n",
    "    train_years = max_train_year - min_train_year + 1\n",
    "    n_days = np.max(X.shape)\n",
    "\n",
    "    train_days = int((n_days/total_years)*train_years)\n",
    "\n",
    "    train_x, train_y = X[:train_days], Y[:train_days]\n",
    "\n",
    "    test_x, test_y = X[train_days:], Y[train_days:]\n",
    "\n",
    "    time_steps = 4\n",
    "    batch_size1 = 15\n",
    "    train_generator = prep.sequence.TimeseriesGenerator(\n",
    "        train_x, \n",
    "        train_y.reshape(-1, projection_dimensions[0], projection_dimensions[1], 1),\n",
    "        length=time_steps, \n",
    "        batch_size=batch_size1\n",
    "        )\n",
    "    test_generator = prep.sequence.TimeseriesGenerator(\n",
    "        test_x, \n",
    "        test_y.reshape(-1, projection_dimensions[0], projection_dimensions[1], 1),\n",
    "        length=time_steps, \n",
    "        batch_size=batch_size1\n",
    "        )\n",
    "\n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clstm_model, train_generator, test_generator, load_weights = False, std_observed = 1.0):\n",
    "\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "    def actual_rmse_loss(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square((y_pred - y_true)*std_observed)))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    \n",
    "    clstm_model.compile(optimizer=adam, loss=root_mean_squared_error, metrics=[root_mean_squared_error, actual_rmse_loss])\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"convlstm_weights_pr.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=f\"./Graphs/norm_csltm_pre_Graph\", histogram_freq=0, write_graph=True, write_images=False)\n",
    "    termnan = tf.keras.callbacks.TerminateOnNaN()\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_delta=0.005, min_lr=0.000004, verbose=1)\n",
    "    \n",
    "    callbacks_list = [checkpoint,tensorboard, reduce_lr, termnan]\n",
    "    \n",
    "    history = clstm_model.fit(\n",
    "        train_generator, \n",
    "        callbacks=callbacks_list, \n",
    "        epochs=32, \n",
    "        validation_data=test_generator,\n",
    "        verbose=1\n",
    "        )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print([x.name for x in local_device_protos if x.device_type == 'GPU'])\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_ConvLSTM_model = AugementedConvLSTM(\n",
    "    projection_height = projection_dimensions[0], \n",
    "    projection_width = projection_dimensions[1],\n",
    "    timesteps=4\n",
    "    )\n",
    "model = Aug_ConvLSTM_model.model(\n",
    "    [32, 16, 16], \n",
    "    [9,5,3], \n",
    "    [64,32,1], \n",
    "    [9,3,5], \n",
    "    2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  def __init__(self, list_examples, batch_size=64, dim=(802, 80),\n",
    "                n_classes=2, shuffle=True):\n",
    "    # Constructor of the data generator.\n",
    "    self.dim = dim\n",
    "    self.batch_size = batch_size\n",
    "    self.list_examples = list_examples\n",
    "    self.n_classes = n_classes\n",
    "    self.shuffle = shuffle\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __len__(self):\n",
    "    # Denotes the number of batches per epoch\n",
    "    return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Generate one batch of data\n",
    "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "    # Find list of IDs\n",
    "    list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "    # Generate data\n",
    "    X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    # This function is called at the end of each epoch.\n",
    "    self.indexes = np.arange(len(self.list_examples))\n",
    "    if self.shuffle == True:\n",
    "      np.random.shuffle(self.indexes)\n",
    "\n",
    "  def __data_generation(self, list_IDs_temp):\n",
    "    # Load individual numpy arrays and aggregate them to a batch.\n",
    "    \n",
    "    X = np.empty([self.batch_size, self.dim[0], self.dim[1]], dtype=np.float32)\n",
    "    \n",
    "    # y is a one-hot encoded vector.\n",
    "    y = np.empty([self.batch_size, self.n_classes], dtype=np.int16)\n",
    "\n",
    "    # Generate data.\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "\n",
    "        # Load sample\n",
    "        X[i,:, :] = np.load(ID[0])\n",
    "        # Load labels       \n",
    "        y[i, :] = np.load(ID[1])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Natural Sort\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "    \n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This loads data for the training set.\n",
    "\"\"\"\n",
    "import glob\n",
    "import random\n",
    "\"\"\"\n",
    "Load the individual numpy arrays into partition\n",
    "\"\"\"\n",
    "data = glob.glob(os.path.join('Data', 'MIROC-ESM', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(data)\n",
    "\n",
    "labels = glob.glob(os.path.join('Data', 'IMD', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(labels)\n",
    "\n",
    "train_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(train_examples)\n",
    "\n",
    "partition = {}\n",
    "partition['train'] = train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "This loads data for the validation set.\n",
    "\"\"\"\n",
    "import glob\n",
    "import random\n",
    "\n",
    "data = glob.glob(os.path.join('Data', 'MIROC-ESM', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(data)\n",
    "\n",
    "labels = glob.glob(os.path.join('Data', 'IMD', \"[0-9]*.npy\"), recursive=True)\n",
    "sort_nicely(labels)\n",
    "\n",
    "validation_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(validation_examples)\n",
    "\n",
    "partition['validation'] = validation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (802, 80),\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 2,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Define the generators\n",
    "training_generator = DataGenerator(partition['train'], **params)\n",
    "validation_generator = DataGenerator(partition['validation'], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for xx, yy in itertools.islice(training_generator, 0, 1, 1):\n",
    "    print(xx.shape, yy.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('Sarth-tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7045c220f384a918686f71b3c00acdc45661fbf73a30bf379cbbfb71cfa47811"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
